exp_dir:
hydra:
  run:
    dir: ${exp_dir}/logs/${now:%Y-%m-%d}/${now:%H-%M-%S}  # Force Hydra output (logs, etc.) into your experiment dir

model:
  model_type: clap  # registered in auden.auto.auto_{config,model}
  audio_encoder:
    model_type: zipformer        # encoder backbone
    pretrained_model: null # optional HF/local path or model id
    frozen: False                # True to freeze encoder params
  text_encoder: 
    model_type: bert             # HF text encoder
    pretrained_model: bert-base-uncased  # optional HF/local path
    frozen: False                # True to freeze text encoder

trainer:
  optimizer: 
    type: scaled_adam
    lr: 0.045
  scheduler: 
    type: eden
    lr_epochs: 3.5 # Number of epochs that affects how rapidly the learning rate decreases.
    lr_batches: 7500 # Number of steps that affects how rapidly the learning rate decreases. We suggest not to change this.
    warmup_batches: 500 # lr warmup steps
    lr_steps_per_epoch: 0 # recommend to adjust this value when you do use_infinite_dataset=True to get exact learning rate schedule as usual. Set it close to your estimated number of steps per epoch
  start_epoch: 1
  num_epochs: 30 # maximum epochs to stop
  start_batch: 0
  num_steps: 300000 # maximum total steps to stop
  ref_duration: 600 # Reference batch duration for purposes of adjusting batch counts for setting various schedules inside the model
  keep_last_k: 30 # save last_k checkpoints on disk
  use_averaged_model: True
  log_interval: 50
  average_period: 200 # how rapidly the model_avg is averaged and saved
  reset_interval: 200 # moving average interval for info tracker
  valid_interval: 1000
  save_every_n: 4 # save checkpoint every (n * valid_interval) steps
  use_fp16: False
  tensorboard: True

  gather_embeddings: True # True to gather embeddings across all ranks

data:
  train_data_config: configs/train_data_config.yaml
  valid_data_config: configs/valid_data_config.yaml
  on_the_fly_feats: True
  sampling_rate: 16000
  use_infinite_dataset: True # the iterator of each dataset will never be exhausted so there will only be steps, no epochs
  data_augmentation:
    enable_spec_aug: True
    enable_musan: False
    musan: null
  sampler:
    type: bucketing_sampler
    num_buckets: 30
    max_duration: 600 # total secs of speech within a minibatch
    shuffle: True
    drop_last: True
  truncate_long_utt: True
  utt_max_duration: 30
  label_field: caption # REQUIRED: field name where captions are stored, e.g., supervision.custom.caption