# Data & Lhotse Manifests (Voice Multitask)

This guide explains how to prepare datasets and Lhotse CutSet manifests for the **Auden-Voice** examples.  
We provide manifests for training the Auden-Voice encoder, but **you must download the datasets yourself** and update absolute paths as described below.

---

## Manifest Format

The model expects **Lhotse CutSet manifests** (`.jsonl.gz`), where each cut contains:

- Audio recording information  
- Supervision metadata with attributes: `speaker`, `emotion`, `gender`, `age_group`

Example supervision structure:

```json
{
  "id": "1077_TAI_DIS_XX-0",
  "start": 0,
  "duration": 2.7694375,
  "channel": 0,
  "supervisions": [
    {
      "id": "1077_TAI_DIS_XX",
      "recording_id": "1077_TAI_DIS_XX",
      "start": 0.0,
      "duration": 2.7694375,
      "channel": 0,
      "speaker": "1077",
      "gender": "male",
      "custom": {
        "age": "20",
        "emotion": "disgust",
        "age_group": "young adult"
      }
    }
  ],
  "recording": {
    "id": "1077_TAI_DIS_XX",
    "sources": [
      {
        "type": "file",
        "channels": [0],
        "source": "/path/CREMA-D/AudioWAV/1077_TAI_DIS_XX.wav"
      }
    ],
    "sampling_rate": 16000,
    "num_samples": 44311,
    "duration": 2.7694375,
    "channel_ids": [0]
  },
  "type": "MonoCut"
}
```

### Dataset Download
⚠️ Please follow each dataset’s license terms and cite the corresponding papers properly.

### 1. CREMA-D

Download instructions: [CREMA-D GitHub](https://github.com/CheyneyComputerScience/CREMA-D). Use `git lfs` to download large audio files.  Only the `AudioWAV` folder is used.

```bash
CREMA-D/
 └── AudioWAV/
      ├── 1001_DFA_ANG_XX.wav
      ├── 1077_TAI_DIS_XX.wav
      └── ...
```

License:  CREMA-D is distributed under the [Open Database License (ODbL)](http://opendatacommons.org/licenses/odbl/1.0/).


---

### 2. RAVDESS

Download instructions: [Zenodo Record](https://zenodo.org/records/1188976). We only need `Audio_Speech_Actors_01-24.zip` (no video files):
  
```bash
wget -c https://zenodo.org/records/1188976/files/Audio_Speech_Actors_01-24.zip
unzip Audio_Speech_Actors_01-24.zip -d RAVDESS_speech
```

Expected structure:

```bash
RAVDESS_speech/
 ├── Actor_01/
 │   ├── 03-01-01-01-01-01-01.wav
 │   ├── 03-01-01-01-02-01-01.wav
 │   └── ...
 ├── Actor_02/
 └── Actor_24/
```
License: [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/)

---


### 3. TESS

Download via [Scholars Portal Dataverse](https://utoronto.scholaris.ca/collections/036db644-9790-4ed0-90cc-be1dfb8a4b66).   Unzip the downloaded `dataverse_files.zip`. Expected structure:

```bash
TESS/
 ├── MANIFEST.TXT
 ├── OAF_back_angry.wav
 ├── OAF_back_disgust.wav
 └── ...
```
License: [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/)

---

#### 4. IEMOCAP
Request access from [USC SAIL](https://sail.usc.edu/iemocap/iemocap_release.htm) by completing the release form.  
After extracting, the folder structure should look like:

```
IEMOCAP_full_release/
├── Session1/
│   └── sentences/wav/
│       ├── Ses01F_impro01/
│           ├── Ses01F_impro01_F000.wav
│           └── ...
│       ├── Ses01M_impro01/
│       └── ...
├── Session2/
│   └── sentences/wav/
│       └── ...
```

License: [IEMOCAP Data Release Form](https://sail.usc.edu/iemocap/Data_Release_Form_IEMOCAP.pdf)


### 5. VoxCeleb2

Download **VoxCeleb2 dev** set from [KAIST MM Lab](https://mm.kaist.ac.kr/datasets/voxceleb/).  
Conversion from `.m4a` → `.wav` requires `ffmpeg`:

```bash
python convert_voxceleb2_m4a_to_wav.py
```

Expected structure:

```bash
voxceleb2/
├── dev/
│   ├── aac/
│   │   └── id00012/abc123/00001.m4a
│   └── wav/   # converted output
│       └── id00012/abc123/00001.wav
```

License: [Creative Commons BY 4.0](https://creativecommons.org/licenses/by/4.0/)


---

## Manifest Preparation

After all datasets are downloaded and organized, replace the manifest placeholder paths with your actual local paths.

Edit the `USER_PATHS` section run the script `python manifests/utils/replace_manifest_path.py`:

```python
USER_PATHS = {
    "CREMA": "/home/username/datasets/CREMA-D",
    "RAVDESS": "/home/username/datasets/RAVDESS_speech",
    "TESS": "/home/username/datasets/TESS",
    "IEMOCAP": "/home/username/datasets/IEMOCAP_full_release",
    "VoxCeleb2": "/home/username/datasets/vox2",
}
```

**Notes on Provided Manifests**

- Label normalization: Emotion labels are standardized across datasets (e.g., `"angry"`, `"happy"`, `"sad"`).
- Age normalization:  Ages are grouped into:
   - `teenager` (<18)  
   - `young adult` (18–39)  
   - `middle-age adult` (40–60)  
   - `senior` (>60)

- Pseudo labels: Some datasets lack emotion or age annotations. We provide pseudo labels generated by internal models.  These manifests are marked accordingly (e.g., `RAVDESS_train_pseudo_age.jsonl.gz`). We are not responsible for the pseudo label quality beyond this example.

- Using custom data to fine-tune with your own dataset:
   - Follow the same JSON manifest structure.  
   - Each supervision must include `speaker`, `gender`, `emotion`, and `age_group`.  
   - Missing values should be `"null"`, and will be handled as "Null" internally in our data module.
   - Update label mapping files under `configs/id2_label_*.json`.

---


### Data Configuration

Update the following files with your manifest paths:
- `configs/data_configs/train_data_config.yaml`: Training datasets
- `configs/data_configs/valid_data_config.yaml`: Validation datasets
- `configs/data_configs/test_data_config.yaml`: Test datasets

### Label Files

Ensure you have label mapping files in `configs/`:
- `id2label_id.json`: Speaker ID labels
- `id2label_emotion.json`: Emotion labels
- `id2label_gender.json`: Gender labels
- `id2label_age.json`: Age group labels