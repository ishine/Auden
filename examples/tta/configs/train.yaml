exp_dir:
hydra:
  run:
    dir: ${exp_dir}/logs/${now:%Y-%m-%d}/${now:%H-%M-%S}

tokenizer: your_path_to_tokenizer_dir

model:
  model_type: tta
  speech_encoder:
    model_type: zipformer
    pretrained_model: null
    frozen: false
  text_encoder:
    model_type: bert
    pretrained_model: bert-base-multilingual-uncased
    frozen: true
  special_tokens:
    ["<transcribe>", "<translate>",
     "<zh>", "<ja>", "<ko>", "<en>", "<fr>", "<es>", "<pt>", "<vi>", "<id>", "<ru>",
     ]

trainer:
  optimizer:
    type: "scaled_adam"
    lr: 0.035
  scheduler:
    type: "eden"
    lr_epochs: 3.5
    lr_batches: 7500
    warmup_batches: 500
    lr_steps_per_epoch: 0
  num_epochs: 30
  start_epoch: 1
  num_steps: 3000000
  start_batch: 0
  rnnt_warm_step: 2000
  prune_range: 5
  lm_scale: 0.25
  am_scale: 0
  simple_loss_scale: 0.5
  attention_loss_scale: 1.0
  s2t_align_loss_scale: 0.1
  forward_attention_decoder_step: 250000 # step to forward attention decoder
  forward_s2t_alignment_step: 250000 # step to forward s2t alignment
  ref_duration: 600
  keep_last_k: 30
  use_averaged_model: true
  log_interval: 50
  average_period: 200
  reset_interval: 200
  valid_interval: 1000
  save_every_n: 4
  use_fp16: true
  tensorboard: true

data:
  train_data_config: configs/train_data_config.yaml
  valid_data_config: configs/valid_data_config.yaml
  on_the_fly_feats: true
  sampling_rate: 16000
  num_workers: 8
  use_infinite_dataset: true
  data_augmentation:
    enable_spec_aug: true
    enable_musan: false
    musan: your_path_to_musan_cuts.jsonl.gz
    enable_speed_perturb: false
  sampler:
    type: bucketing_sampler
    num_buckets: 30
    max_duration: 250
    shuffle: true
    drop_last: true
  text_normalization: true


