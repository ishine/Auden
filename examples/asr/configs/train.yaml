exp_dir:
hydra:
  run:
    dir: ${exp_dir}/logs/${now:%Y-%m-%d}/${now:%H-%M-%S}  # Force Hydra output (logs, etc.) into your experiment dir

tokenizer: your_path_to_tokenizer_dir

model:
  model_type: asr  # Required
  # Provide either a path/HF repo to a directory with config.json for the encoder,
  # or rely on model code defaults (ZipformerConfig()). If you need to override,
  # pass encoder_config via script.
  encoder: 
    model_type: zipformer
    pretrained_encoder: null

trainer:
  optimizer: 
    type: "scaled_adam"
    lr: 0.045
  scheduler: 
    type: "eden"
    lr_epochs: 3.5 # Number of epochs that affects how rapidly the learning rate decreases.
    lr_batches: 7500 # Number of steps that affects how rapidly the learning rate decreases. We suggest not to change this.
    warmup_batches: 500 # lr warmup steps
    lr_steps_per_epoch: 0 # recommend to adjust this value when you do use_infinite_dataset=True to get exact learning rate schedule as usual. Set it close to your estimated number of steps per epoch
  num_epochs: 30
  start_epoch: 1
  num_steps: 3000000
  start_batch: 0
  keep_last_k: 30 # save last_k checkpoints on disk
  use_averaged_model: True
  log_interval: 50
  average_period: 200 # how rapidly the averaged_model is averaged and saved
  reset_interval: 200 # moving average interval for info tracker
  valid_interval: 1000
  save_every_n: 4 # save checkpoint every (n * valid_interval) steps
  use_fp16: True
  tensorboard: True

  prune_range: 5 # The prune range for rnnt loss, it means how many symbols(context) we are using to compute the loss
  lm_scale: 0.25 # The scale to smooth the loss with lm(output of prediction network) part
  am_scale: 0 #The scale to smooth the loss with am (output of encoder network) part
  simple_loss_scale: 0.5
  rnnt_warm_step: 2000

data:
  train_data_config: configs/data_configs/train_data_config.yaml
  valid_data_config: configs/data_configs/valid_data_config.yaml
  on_the_fly_feats: True
  sampling_rate: 16000
  num_workers: 8
  pad_to_30s: False # True for original Whisper encoders
  use_infinite_dataset: True # the iterator of each dataset will never be exhausted so there will only be steps, no epochs
  data_augmentation:
    enable_spec_aug: True
    enable_musan: False
    musan: your_path_to_musan_cuts.jsonl.gz
    enable_speed_perturb: False
  sampler:
    type: bucketing_sampler
    num_buckets: 30
    max_duration: 600 # total secs of speech within a minibatch
    shuffle: True
    drop_last: True
  text_normalization: True